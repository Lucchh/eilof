# -*- coding: utf-8 -*-
"""An Efficient Outlier Detection Algorithm for Data Streaming

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B7OujN2EcG7IzPIgw_WxkKk-qH3VxqLA
"""

from scipy.spatial.distance import cdist
from sklearn.metrics import f1_score, accuracy_score
from matplotlib.ticker import MaxNLocator
from google.colab import files
import numpy as np
import random
import os
import matplotlib.pyplot as plt
import seaborn as sns

def k_nearest_neighbors(dist_matrix, k):
    """ Returns the indices of k nearest neighbors for each point, excluding the point itself. """
    num_points = dist_matrix.shape[0]
    neighbors = []

    for i in range(num_points):
        # Sort distances for the i-th point and get indices
        sorted_neighbors = np.argsort(dist_matrix[i])

        # Filter out the point itself from its list of neighbors
        filtered_neighbors = sorted_neighbors[sorted_neighbors != i]

        # Take the first k neighbors
        neighbors.append(filtered_neighbors[:k])
    # neighbors = np.argpartition(dist_matrix, k, axis=1)[:, :k]

    return np.array(neighbors)

def reachability_distance(dist_matrix, k):
    """ Computes the reachability distance """
    reachability_dist_matrix = np.zeros_like(dist_matrix)
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]
    for i in range(len(dist_matrix)):
        for j in range(len(dist_matrix)):
            if i == j:
                continue
            else:
                kth_dist_neighbor = kth_distances[j]
                reachability_dist_matrix[i, j] = max(dist_matrix[i, j], kth_dist_neighbor)
    return reachability_dist_matrix

def local_reachability_density(reachability_dist, neighbors):
    return 1 / np.mean(reachability_dist[np.arange(len(reachability_dist))[:, None], neighbors], axis = 1)


def lof_srs(dist_matrix, neighbors, lrd, k):
    num_points = len(lrd)
    LOF_list = np.zeros(num_points)
    for i in range(num_points):
        lof_sum = 0
        for neighbor in neighbors[i]:
            lof_sum += lrd[neighbor]
        LOF_list[i] = (lof_sum / k) / lrd[i]
    return LOF_list

def local_outlier_factor(data, k):
    """ Main function to compute Local Outlier Factor """
    dist_matrix = cdist(data, data, 'euclidean')
    neighbors = k_nearest_neighbors(dist_matrix, k) #kth nearest distance of the point
    reachability_dist = reachability_distance(dist_matrix, k)
    lrd = local_reachability_density(reachability_dist, neighbors)
    lof_scores = lof_srs(dist_matrix, neighbors, lrd, k)
    return lof_scores

"""ILOF Algorithm"""

def compute_distances(updated_data, new_point):
    """Computes the Euclidean distances of a new point to all existing points in a dataset."""
    distances = np.sqrt(np.sum((updated_data - new_point) ** 2, axis=1))
    return distances

def compute_reachability_distance(dist_matrix, k):
    """Compute the reachability distance for each point."""
    reach_dist_matrix = np.zeros_like(dist_matrix)
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]
    for i in range(len(dist_matrix)):
        for j in range(len(dist_matrix)):
            if i != j:
                reach_dist_matrix[i, j] = max(dist_matrix[i, j], kth_distances[j])
    return reach_dist_matrix

def incremental_lof(updated_data, k, new_point, reachability_dist):
    dist_matrix = cdist(updated_data, updated_data, 'euclidean') #orginal distance matrix
    k_th_dist_og = np.sort(dist_matrix, axis=1)[:, k]

    distance_new = compute_distances(updated_data, new_point) #calculate the distance of new point to other points

    updated_data = np.append(updated_data, [new_point], axis=0) #update the data
    k_nearest_neib_new = np.sort(distance_new)[k-1]
    k_nearest_index = np.argsort(distance_new)[k-1]
    # Compute reachability distance from new point to its k-nearest neighbors
    dist_matrix_new = cdist(updated_data, updated_data, 'euclidean') #new distance matrix
    # Compute the sorted indices (k-nearest neighbors) for the new point
    k_nearest_indices_new = np.argsort(dist_matrix_new)[:, 1:k+1][-1]

    neighbors_new = k_nearest_neighbors(dist_matrix_new, k)
    kth_distances_new = np.sort(dist_matrix_new, axis=1)[:, k]
    reachability_distance_new = []
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]

    reachability_dist = np.pad(reachability_dist, ((0, 1), (0, 1)), mode='constant', constant_values=0)

    for i in k_nearest_indices_new:
        reach_dist = max(kth_distances_new[i], distance_new[i])
        reachability_distance_new.append(reach_dist)
        reachability_dist[-1, i] = reach_dist


    #get the KRNN
    KRNN = np.nonzero(distance_new < k_th_dist_og)[0]
    for i in KRNN:
        # Check the condition to update k-distance based on the new point
        if distance_new[i] < np.sort(dist_matrix, axis=1)[i][k] and distance_new[i] > np.sort(dist_matrix, axis=1)[i][k-1]:
            k_th_dist_og[i] = distance_new[i]
        elif distance_new[i] <= np.sort(dist_matrix, axis=1)[i][k-1]:
            k_th_dist_og[i] = np.sort(dist_matrix, axis=1)[i][k-1]
    K_update_dis = np.append(k_th_dist_og, k_nearest_neib_new)
    S_update_lrd = set()

    S_update_lrd = set(KRNN)
    for pj in KRNN:
        # Get the k-nearest neighbors of pj excluding the new point
        neighbors_pj = [neighbor for neighbor in neighbors_new[pj] if neighbor != len(updated_data)]

        # Update reachability distances for pj and its neighbors
        for pi in neighbors_pj:
            # Calculate the reachability distance between pj and pi
            actual_dist = dist_matrix_new[pi][pj]
            k_distance_pj = K_update_dis[pj]
            reachability_dist[pi][pj] = max(actual_dist, k_distance_pj)

            # If pj is a k-nearest neighbor of pi, add pi to S_update_lrd
            if pj in neighbors_new[pi]:
                S_update_lrd.add(pi)
    # Convert S_update_lrd to a list
    S_update_lrd = list(S_update_lrd)
    value_to_remove = len(updated_data) - 1
    if value_to_remove in S_update_lrd:
        S_update_lrd.remove(value_to_remove)


    S_update_LOF = set(S_update_lrd)




    # Update the LRD for points in S_update_lrd
    for pm in S_update_lrd:
        neighbors_pm = neighbors_new[pm]
        # Exclude the new point from the neighbors, if present
        # Recalculate LRD for pm
        reach_dist_sum_pm = 0
        for neighbor in neighbors_pm:
            if neighbor  == len(updated_data) - 1 and distance_new[pm] < k_nearest_neib_new:
              reach_dist_sum_pm += k_nearest_neib_new
            elif neighbor  == len(updated_data) - 1 and distance_new[pm] >= k_nearest_neib_new:
              reach_dist_sum_pm += distance_new[pm]
            elif reachability_dist[pm][neighbor] == 0:
              reachability_dist[pm][neighbor] = max(dist_matrix_new[pm][neighbor],K_update_dis[neighbor])
              reach_dist_sum_pm += reachability_dist[pm][neighbor]
            # elif pm > len(selected_data):
            #   reach_dist_sum_pm += combined_dict[neighbor]
            else:
              reach_dist_sum_pm += reachability_dist[pm][neighbor]
        lrd_og[pm] = 1.0 / (reach_dist_sum_pm / k)
        # Go through each point's neighbors to see if pm is a k-nearest neighbor
        # for pi, neighbors in enumerate(neighbors_og):
        #     if pm in neighbors:
        #         kRNN.append(pi)

    KRNN = []
    for index, row in enumerate(neighbors_new):
        if any(value in row for value in S_update_lrd):
            KRNN.append(index)

    # Add the kRNN to the S_update_LOF set
    S_update_LOF.update(KRNN)

    # Convert S_update_LOF to a list
    S_update_LOF = list(S_update_LOF)


    lrd_pc = 1.0 / (sum(reachability_distance_new) / k)
    lrd_sum_all = np.append(lrd_og, lrd_pc)

    # Update the LOF for each point in S_update_LOF
    for pi in S_update_LOF:
        if pi != len(updated_data) - 1:
            neighbors_pi = neighbors_new[pi]
            sum_lrd_ratios = sum(lrd_sum_all[neighbor] for neighbor in neighbors_pi) / k
            lof_og[pi] = sum_lrd_ratios / lrd_sum_all[pi]
    average_lrd_neighbors = sum(lrd_og[neighbor] for neighbor in k_nearest_indices_new) / k
    lrd_sum_neighbors = average_lrd_neighbors / lrd_pc



    # final
    return (np.append(lof_og, lrd_sum_neighbors)), reachability_dist, lrd_sum_all

"""EILOF Algorithm

"""

def compute_distances(updated_data, new_point):
    """Computes the Euclidean distances of a new point to all existing points in a dataset."""
    distances = np.sqrt(np.sum((updated_data - new_point) ** 2, axis=1))
    return distances

def compute_reachability_distance(dist_matrix, k):
    """Compute the reachability distance for each point"""
    reach_dist_matrix = np.zeros_like(dist_matrix)
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]
    for i in range(len(dist_matrix)):
        for j in range(len(dist_matrix)):
            if i != j:
                reach_dist_matrix[i, j] = max(dist_matrix[i, j], kth_distances[j])
    return reach_dist_matrix

def incremental_lof_update_new(updated_data, k, new_point, reachability_dist):
    dist_matrix = cdist(updated_data, updated_data, 'euclidean') #orginal distance matrix
    k_th_dist_og = np.sort(dist_matrix, axis=1)[:, k]
    distance_new = compute_distances(updated_data, new_point) #calculate the distance of new point to other points
    updated_data = np.append(updated_data, [new_point], axis=0) #update the data
    # kth distance of the new point
    k_nearest_neib_new = np.sort(distance_new)[k-1]
    # Compute reachability distance from new point to its k-nearest neighbors
    dist_matrix = cdist(updated_data, updated_data, 'euclidean') #new distance matrix
    # Compute the sorted indices (k-nearest neighbors) for the new point
    k_nearest_indices_new = np.argsort(dist_matrix)[:, 1:k+1][-1]

    neighbors_new = k_nearest_neighbors(dist_matrix, k)
    kth_distances_new = np.sort(dist_matrix, axis=1)[:, k]
    reachability_distance_new = []
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]

    reachability_dist = np.pad(reachability_dist, ((0, 1), (0, 1)), mode='constant', constant_values=0)

    lrd_update_list = []
    for i in k_nearest_indices_new:
          reach_dist = max(kth_distances_new[i], distance_new[i])
          reachability_distance_new.append(reach_dist)
          reachability_dist[-1, i] = reach_dist
          if len(data_copy)-1 in neighbors_new[i]:
             lrd_update_list.append(i)
             reachability_dist[i, -1] = max(k_nearest_neib_new, distance_new[i])


    for i in lrd_update_list:
        lrd_og[i] = 1.0 / (sum(reachability_dist[i][neighbors_new[i]])/ k)


    lrd_pc = 1.0 / (sum(reachability_distance_new) / k)
    lrd_sum_all = np.append(lrd_og, lrd_pc)
    average_lrd_neighbors = sum(lrd_og[neighbor] for neighbor in k_nearest_indices_new) / k
    lrd_sum_neighbors = average_lrd_neighbors / lrd_pc

    # final
    return (np.append(lof_og, lrd_sum_neighbors)), reachability_dist, lrd_sum_all









"""Trying to optimize EILOF (test phase)"""

def compute_distances(updated_data, new_point):
    """Computes the Euclidean distances of a new point to all existing points in a dataset."""
    distances = np.sqrt(np.sum((updated_data - new_point) ** 2, axis=1))
    return distances

def compute_reachability_distance(dist_matrix, k):
    """Compute the reachability distance for each point"""
    reach_dist_matrix = np.zeros_like(dist_matrix)
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]
    for i in range(len(dist_matrix)):
        for j in range(len(dist_matrix)):
            if i != j:
                reach_dist_matrix[i, j] = max(dist_matrix[i, j], kth_distances[j])
    return reach_dist_matrix

def incremental_lof_update_new(updated_data, k, new_point, reachability_dist, dist_matrix):
    distance_new = compute_distances(updated_data, new_point)
    # Incrementally update the distance matrix
    dist_matrix = np.hstack((dist_matrix, distance_new.reshape(-1, 1)))
    dist_new_row = np.append(distance_new, 0).reshape(1, -1)
    dist_matrix = np.vstack((dist_matrix, dist_new_row))
    k_th_dist_og = np.sort(dist_matrix, axis=1)[:, k]
    distance_new = compute_distances(updated_data, new_point) #calculate the distance of new point to other points
    updated_data = np.append(updated_data, [new_point], axis=0) #update the data
    # kth distance of the new point
    k_nearest_neib_new = np.sort(distance_new)[k-1]
    # Compute reachability distance from new point to its k-nearest neighbors
    dist_matrix = cdist(updated_data, updated_data, 'euclidean') #new distance matrix
    # Compute the sorted indices (k-nearest neighbors) for the new point
    k_nearest_indices_new = np.argsort(dist_matrix)[:, 1:k+1][-1]

    neighbors_new = k_nearest_neighbors(dist_matrix, k)
    kth_distances_new = np.sort(dist_matrix, axis=1)[:, k]
    reachability_distance_new = []
    kth_distances = np.sort(dist_matrix, axis=1)[:, k]

    reachability_dist = np.pad(reachability_dist, ((0, 1), (0, 1)), mode='constant', constant_values=0)

    lrd_update_list = []
    for i in k_nearest_indices_new:
          reach_dist = max(kth_distances_new[i], distance_new[i])
          reachability_distance_new.append(reach_dist)
          reachability_dist[-1, i] = reach_dist
          if len(data_copy)-1 in neighbors_new[i]:
             lrd_update_list.append(i)
             reachability_dist[i, -1] = max(k_nearest_neib_new, distance_new[i])


    for i in lrd_update_list:
        lrd_og[i] = 1.0 / (sum(reachability_dist[i][neighbors_new[i]])/ k)


    lrd_pc = 1.0 / (sum(reachability_distance_new) / k)
    lrd_sum_all = np.append(lrd_og, lrd_pc)
    average_lrd_neighbors = sum(lrd_og[neighbor] for neighbor in k_nearest_indices_new) / k
    lrd_sum_neighbors = average_lrd_neighbors / lrd_pc

    # final
    return (np.append(lof_og, lrd_sum_neighbors)), reachability_dist, lrd_sum_all, dist_matrix

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,100)
f1_total = []
k = [50]
for kk in k:
    data_copy = selected_data
    lof_list = []
    reach_list = []
    lrd_list = []
    dist_og = cdist(selected_data, selected_data, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, kk)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, kk]
    reachability_dist = reachability_distance(dist_og, kk) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, kk)
    f1_new_list = []
    for i in test_index:
        new_point = unselected_data[i]
        lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, kk, new_point, reachability_dist, dist_og)
        data_copy = np.append(data_copy, [new_point], axis = 0)
        lof_mean_incremental = np.mean(lof_og)
        lof_std_inc = np.std(lof_og)
        threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
        # Classify points based on the threshold
        predicted_labels = (lof_og > threshold_incremental_new).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i+1])
        # Calculate F1 score and accuracy
        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        accuracy = accuracy_score(labels_update, predicted_labels)
        f1_new_list.append(f1)
    f1_new_list

test_index = range(0,1280)
indices = [1,4,9,19,39,79,159,319,639,1279]
f1_total = []
k = [25,50]
for kk in k:
    data_copy = selected_data
    lof_list = []
    reach_list = []
    lrd_list = []
    dist_og = cdist(selected_data, selected_data, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, kk)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, kk]
    reachability_dist = reachability_distance(dist_og, kk) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, kk)
    f1_new_list = []
    for i in test_index:
        new_point = unselected_data[i]
        lof_og, reachability_dist, lrd_og, dist_og = incremental_lof_update_new(data_copy, kk, new_point, reachability_dist, dist_og)
        data_copy = np.append(data_copy, [new_point], axis = 0)
        lof_mean_incremental = np.mean(lof_og)
        lof_std_inc = np.std(lof_og)
        threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
        # Classify points based on the threshold
        predicted_labels = (lof_og > threshold_incremental_new).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i+1])
        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        f1_new_list.append(f1)
    selected_values = [f1_new_list[i] for i in indices]
    f1_total.append(selected_values)

f1_total







"""Simulation Data Generation

"""

np.random.seed(5)
def generate_simulation_data(n_points, outlier_ratio, dimensions, outlier_distance):

    n_outliers = int(n_points * outlier_ratio)
    n_normals = n_points - n_outliers

    # Generate normal data points around a center
    normal_data = np.random.randn(n_normals, dimensions)

    # Generate outliers
    outlier_data = np.random.randn(n_outliers, dimensions) * outlier_distance + outlier_distance

    # Combine normal data and outliers
    data = np.vstack([normal_data, outlier_data])

    # Create labels (0 for normal, 1 for outlier)
    labels = np.array([0] * n_normals + [1] * n_outliers)

    return data, labels

# 2D Data with 5% Outliers and Separation of 10
data, labels = generate_simulation_data(n_points=2280, outlier_ratio=0.05, dimensions=2, outlier_distance = 10)

"""Fig. 2: Distribution of Simulated Data Points"""

# Specify the directory where the file will be saved. Enter your own directory name
save_dir = "..."
os.makedirs(save_dir, exist_ok=True)

# Extract x and y coordinates from the data
x = data[:, 0]
y = data[:, 1]

# Labels array indicating outliers (1 for outliers, 0 for normal points)
outliers = labels == 1

# Set the style for seaborn
sns.set(style="whitegrid")

# Create a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(x[~outliers], y[~outliers], c='blue', alpha=0.6, edgecolors='w', s=80, label='Non-Outliers')
plt.scatter(x[outliers], y[outliers], c='red', alpha=0.6, edgecolors='w', s=80, label='Outliers')
plt.xlabel('Feature 1 (X-axis)', fontsize=12)
plt.ylabel('Feature 2 (Y-axis)', fontsize=12)
plt.legend()
plt.grid(True)
plt.tight_layout()

# Save the plot as an image file in the specified directory
save_path = os.path.join(save_dir, 'Distribution_of_Simulated_Data_Points.png')
plt.savefig(save_path, dpi=300)

# from google.colab import files
# files.download(save_path)

# Display the plot
plt.show()

"""Setting of the simulated dataset"""

random.seed(5)
all_indices = list(range(0, 2280))
selected_index = random.sample(all_indices, 1000)
unselected_index = [index for index in all_indices if index not in selected_index]
random.shuffle(unselected_index)
# Use selected_index and unselected_index to get selected_data, unselected_data, selected_labels, and unselected_labels
selected_data = data[selected_index]
selected_labels = labels[selected_index]
unselected_data = data[unselected_index]
unselected_labels = labels[unselected_index]

"""$F_1$ by Test Index for Different $k$ Values in ILOF"""

test_index = [1,5,10,20,40,80,160,320,640,1280]
k = [25,50,75,100,125,150]
total_f1_list = []
for kk in k:
    f1_list = []
    for i in test_index:
        updated_data = np.append(selected_data, unselected_data[:i], axis=0)
        lof_scores = local_outlier_factor(updated_data, kk)
        lof_mean_og = np.mean(lof_scores)
        lof_std = np.std(lof_scores)
        threshold = lof_mean_og + 2 * lof_std
        # Classify points based on the threshold
        predicted_labels = (lof_scores > threshold).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i])

        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        accuracy = accuracy_score(labels_update, predicted_labels)
        f1_list.append(f1)
    total_f1_list.append(f1_list)

"""0.6933333333333334,
0.6933333333333334,
0.6933333333333334,
0.6842105263157895,
0.6842105263157895,
0.7000000000000001,
0.7058823529411764,
0.6444444444444445,
0.5333333333333333,
0.4161073825503356

Fig. 3: $F_1$ by Test Index for Different $k$ Values in ILOF
"""

# Set the style for the plot to be suitable for formal presentations/publications
plt.style.use('seaborn-whitegrid')

# Set the font properties that will be used for the plot
font = {'family': 'Times New Roman', 'weight': 'normal', 'size': 16}

plt.rc('font', **font)  # Apply the font properties

# Create a figure and axis with the desired size for publication
fig, ax = plt.subplots(figsize=(10, 6))

# Define a color map with a professional appearance
colors = plt.cm.viridis(np.linspace(0, 1, len(k)))

# Create the line plot for each array in total_f1_list
for i, f1_array in enumerate(total_f1_list):
    ax.plot(test_index, f1_array, marker='o', linestyle='-', color=colors[i], label=f'k={k[i]}')

ax.set_xlabel('Incremental Points Added')  # Add x-axis label
ax.set_ylabel('F1 Score')  # Add y-axis label
#ax.set_title('F1 Score by Test Index for Different k Values')  # Add title
ax.legend(title='Parameter k', loc='best', fontsize='small')  # Add a legend with a title
ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Force x-axis to show integer values

# Set the grid to appear behind the plot elements
ax.set_axisbelow(True)

# Set the figure to have a tight layout
plt.tight_layout()

# Save the figure with high resolution
plt.savefig('F1 Score by Test Index for Different k Values.png', dpi=300)  # Save the figure with a DPI of 300
from google.colab import files
files.download('F1 Score by Test Index for Different k Values.png')  # Prompt to download the file to your local machine




plt.show()  # Show the plot as the last step

test_index = [80,160,320,640,1280]
k = [25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,125,130,135,140,145,150,155,160,165,170,175]
total_f1_list = []
for kk in k:
    f1_list_test = []
    for i in test_index:
        updated_data = np.append(selected_data, unselected_data[:i], axis=0)
        testttttt = local_outlier_factor(updated_data, kk)
        lof_mean_og = np.mean(testttttt)
        lof_std = np.std(testttttt)
        threshold = lof_mean_og + 2 * lof_std
        # Classify points based on the threshold
        predicted_label_test = (testttttt > threshold).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i])

        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_label_test)
        accuracy = accuracy_score(labels_update, predicted_label_test)
        f1_list_test.append(f1)
    total_f1_list.append(f1_list_test)

"""Fig. 4: $F_{1}$ Score by $k$ for Different Sizes of Incremental Data Points ($m$) in ILOF"""

import matplotlib.pyplot as plt
import numpy as np

test_index = [80, 160, 320, 640, 1280]
k = [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175]



colors = plt.cm.tab20(np.linspace(0, 1, len(test_index)))

plt.figure(figsize=(12, 8))

line_styles = ['-']
markers = ['o', 's', '^', 'D', 'x', '+', '*', '<', '>', 'p', 'P', 'h', 'H', 'X', '|', '_']

# Create the line plot for each value in test_index
for i, m in enumerate(test_index):
    ls = line_styles[i % len(line_styles)]
    mk = markers[i % len(markers)]
    plt.plot(k, np.array(total_f1_list)[:,i], marker=mk, linestyle=ls, color=colors[i % len(colors)], label=f'm={m}')

plt.xlabel('k Values', fontsize=16)  # Increase font size for x-axis label
plt.ylabel('F1 Score', fontsize=16)  # Increase font size for y-axis label
plt.xticks(fontsize=16)  # Increase font size for x-axis tick labels
plt.yticks(fontsize=16)  # Increase font size for y-axis tick labels

plt.ylim(0.4, 1.0)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), borderaxespad=0., fontsize=16, markerscale=1.5, handlelength=3, handleheight=2, shadow=True, frameon=False)
plt.tight_layout()  # Adjust the padding to fit the legend and prevent clipping
plt.grid(True)


# Save the figure with high resolution
plt.savefig('F1 Score by k for Different Sizes of Incremental Data Points (m).png', dpi=300)  # Save the figure with a DPI of 300
from google.colab import files
files.download('F1 Score by k for Different Sizes of Incremental Data Points (m).png')
plt.show()

"""$F_1$ by Test Index for Different $k$ Values in EILOF"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,1280)
indices = [80,160,320,640,1280]
f1_total = []
k = [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175]
for kk in k:
    data_copy = selected_data
    lof_list = []
    reach_list = []
    lrd_list = []
    dist_og = cdist(selected_data, selected_data, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, kk)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, kk]
    reachability_dist = reachability_distance(dist_og, kk) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, kk)
    f1_new_list = []
    for i in test_index:
        new_point = unselected_data[i]
        lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, kk, new_point, reachability_dist)
        data_copy = np.append(data_copy, [new_point], axis = 0)
        lof_mean_incremental = np.mean(lof_og)
        lof_std_inc = np.std(lof_og)
        threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
        # Classify points based on the threshold
        predicted_labels = (lof_og > threshold_incremental_new).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i+1])
        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        f1_new_list.append(f1)
    selected_values = [f1_new_list[i-1] for i in indices]
    f1_total.append(selected_values)

"""Fig. 6: $F_{1}$ Score by $k$ for Different Sizes of Incremental Data Points ($m$) in EILOF"""

test_index = [80, 160, 320, 640, 1280]
k = [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175]

colors = plt.cm.tab20(np.linspace(0, 1, len(test_index)))

plt.figure(figsize=(12, 8))

line_styles = ['-']
markers = ['o', 's', '^', 'D', 'x', '+', '*', '<', '>', 'p', 'P', 'h', 'H', 'X', '|', '_']

# Create the line plot for each value in test_index
for i, m in enumerate(test_index):
    ls = line_styles[i % len(line_styles)]
    mk = markers[i % len(markers)]
    plt.plot(k, np.array(f1_total)[:,i], marker=mk, linestyle=ls, color=colors[i % len(colors)], label=f'm={m}')

plt.xlabel('k Values', fontsize=16)  # Increase font size for x-axis label
plt.ylabel('F1 Score', fontsize=16)  # Increase font size for y-axis label
plt.xticks(fontsize=16)  # Increase font size for x-axis tick labels
plt.yticks(fontsize=16)  # Increase font size for y-axis tick labels

plt.ylim(0.4, 1.0)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), borderaxespad=0., fontsize=16, markerscale=1.5, handlelength=3, handleheight=2, shadow=True, frameon=False)
plt.tight_layout()  # Adjust the padding to fit the legend and prevent clipping
plt.grid(True)



# Save the figure with high resolution
plt.savefig('F1 Score by k for Different Sizes of Incremental Data Points (m) for EILOF.png', dpi=300)  # Save the figure with a DPI of 300
from google.colab import files
files.download('F1 Score by k for Different Sizes of Incremental Data Points (m) for EILOF.png')
plt.show()

"""Comparison of Incremental $F_{1}$ Scores in $2D$: ILOF vs. EILOF when $k = 50 $"""

test_index = range(0,640)
k = 50
f1_new_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list.append(f1)
f1_new_list

# Number of increments (x-axis)
x = range(len(f1_list))

# Select key points every 20th point for clarity
key_points = np.arange(0, len(f1_list), 20)

# Plotting the data
plt.figure(figsize=(10, 6))

# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list)[key_points], label='ILOF', marker='s', color='blue', markersize=7, linestyle='-', linewidth=1.5)
plt.plot(key_points, np.array(f1_new_list)[key_points], label='EILOF', marker='s', color='green', linestyle='-', markersize=7, linewidth=1.5)


# Adding labels, legend, and grid
plt.xlabel('Incremental Points', fontsize=12)
plt.ylabel('F1 Score', fontsize=12)
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.savefig('incremental_f1_scores_comparison.png', dpi=300)

# Show plot
plt.show()
files.download('incremental_f1_scores_comparison.png')

"""TABLE I: $F_{1}$ Scores for Different Values of $k$ (Number of Neighbors) and $ m $ (Points Added) in 50 dimensions for ILOF"""

np.random.seed(5)
def generate_simulation_data(n_points, outlier_ratio, dimensions, outlier_distance):
    n_outliers = int(n_points * outlier_ratio)
    n_normals = n_points - n_outliers

    # Generate normal data points around a center
    normal_data = np.random.randn(n_normals, dimensions)

    # Calculate the rescaling factor for higher dimensions
    rescaling_factor = np.sqrt(2 / dimensions)  # sqrt(2/dimensions) to match the 2D scaling

    # Rescale the outlier_distance
    rescaled_outlier_distance = outlier_distance * rescaling_factor

    # Generate outliers with rescaled distance
    outlier_data = np.random.randn(n_outliers, dimensions) * rescaled_outlier_distance + rescaled_outlier_distance

    # Combine normal data and outliers
    data = np.vstack([normal_data, outlier_data])

    # Create labels (0 for normal, 1 for outlier)
    labels = np.array([0] * n_normals + [1] * n_outliers)

    return data, labels

# Example usage for 50 dimensions with rescaling
data, labels = generate_simulation_data(n_points=2280, outlier_ratio=0.05, dimensions=50, outlier_distance=10)

random.seed(5)
all_indices = list(range(0, 2280))
selected_index = random.sample(all_indices, 1000)
unselected_index = [index for index in all_indices if index not in selected_index]
random.shuffle(unselected_index)
# Use selected_index and unselected_index to get selected_data, unselected_data, selected_labels, and unselected_labels
selected_data = data[selected_index]
selected_labels = labels[selected_index]
unselected_data = data[unselected_index]
unselected_labels = labels[unselected_index]

test_index = [1,5,10,20,40,80,160,320,640,1280]
k = [25,50,75,100,125,150]
total_f1_list = []
for kk in k:
    f1_list = []
    for i in test_index:
        updated_data = np.append(selected_data, unselected_data[:i], axis=0)
        lof_scores = local_outlier_factor(updated_data, kk)
        lof_mean_og = np.mean(lof_scores)
        lof_std = np.std(lof_scores)
        threshold = lof_mean_og + 2 * lof_std
        # Classify points based on the threshold
        predicted_labels = (lof_scores > threshold).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i])

        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        accuracy = accuracy_score(labels_update, predicted_labels)
        f1_list.append(f1)
    total_f1_list.append(f1_list)

total_f1_list

"""TABLE II: $F_{1}$ Scores for Different Values of $k$ (Number of Neighbors) and $ m $ (Points Added) in 2 dimensions for EILOF"""

np.random.seed(5)
def generate_simulation_data(n_points, outlier_ratio, dimensions, outlier_distance):
    n_outliers = int(n_points * outlier_ratio)
    n_normals = n_points - n_outliers

    # Generate normal data points around a center
    normal_data = np.random.randn(n_normals, dimensions)

    # Calculate the rescaling factor for higher dimensions
    rescaling_factor = np.sqrt(2 / dimensions)  # sqrt(2/dimensions) to match the 2D scaling

    # Rescale the outlier_distance
    rescaled_outlier_distance = outlier_distance * rescaling_factor

    # Generate outliers with rescaled distance
    outlier_data = np.random.randn(n_outliers, dimensions) * rescaled_outlier_distance + rescaled_outlier_distance

    # Combine normal data and outliers
    data = np.vstack([normal_data, outlier_data])

    # Create labels (0 for normal, 1 for outlier)
    labels = np.array([0] * n_normals + [1] * n_outliers)

    return data, labels


data, labels = generate_simulation_data(n_points=2280, outlier_ratio=0.05, dimensions=2, outlier_distance=10)

random.seed(5)
all_indices = list(range(0, 2280))
selected_index = random.sample(all_indices, 1000)
unselected_index = [index for index in all_indices if index not in selected_index]
random.shuffle(unselected_index)
# Use selected_index and unselected_index to get selected_data, unselected_data, selected_labels, and unselected_labels
selected_data = data[selected_index]
selected_labels = labels[selected_index]
unselected_data = data[unselected_index]
unselected_labels = labels[unselected_index]

test_index = range(0,1280)
indices = [1,4,9,19,39,79,159,319,639,1279]
f1_total = []
k = [25,50,75,100,125,150]
for kk in k:
    data_copy = selected_data
    lof_list = []
    reach_list = []
    lrd_list = []
    dist_og = cdist(selected_data, selected_data, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, kk)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, kk]
    reachability_dist = reachability_distance(dist_og, kk) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, kk)
    f1_new_list = []
    for i in test_index:
        new_point = unselected_data[i]
        lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, kk, new_point, reachability_dist)
        data_copy = np.append(data_copy, [new_point], axis = 0)
        lof_mean_incremental = np.mean(lof_og)
        lof_std_inc = np.std(lof_og)
        threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
        # Classify points based on the threshold
        predicted_labels = (lof_og > threshold_incremental_new).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i+1])
        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        f1_new_list.append(f1)
    selected_values = [f1_new_list[i] for i in indices]
    f1_total.append(selected_values)



f1_total

plt.style.use('seaborn-whitegrid')

# Set the font properties that will be used for the plot
font = {'family': 'Times New Roman', 'weight': 'normal', 'size': 16}

plt.rc('font', **font)  # Apply the font properties

# Create a figure and axis with the desired size for publication
fig, ax = plt.subplots(figsize=(10, 6))

# Define a color map with a professional appearance
colors = plt.cm.viridis(np.linspace(0, 1, len(k)))

# Create the line plot for each array in f1_total
for i, f1_array in enumerate(f1_total):
    ax.plot(test_index, f1_array, marker='o', linestyle='-', color=colors[i], label=f'k={k[i]}')

ax.set_xlabel('Incremental Points Added')  # Add x-axis label
ax.set_ylabel('F1 Score')  # Add y-axis label
#ax.set_title('F1 Score by Test Index for Different k Values')  # Add title
ax.legend(title='Parameter k', loc='best', fontsize='small')  # Add a legend with a title
ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # Force x-axis to show integer values
ax.set_ylim(0.4, 1)
# Set the grid to appear behind the plot elements
ax.set_axisbelow(True)

# Set the figure to have a tight layout
plt.tight_layout()

# Save the figure with high resolution
plt.savefig('F1 Score by Test Index for Different k Values.png', dpi=300)  # Save the figure with a DPI of 300
from google.colab import files
files.download('F1 Score by Test Index for Different k Values.png')  # Prompt to download the file to your local machine




plt.show()  # Show the plot as the last step

"""TABLE II: $F_{1}$ Scores for Different Values of $k$ (Number of Neighbors) and $ m $ (Points Added) in 50 dimensions for EILOF"""

np.random.seed(5)

def generate_simulation_data(n_points, outlier_ratio, dimensions, outlier_distance):
    n_outliers = int(n_points * outlier_ratio)
    n_normals = n_points - n_outliers

    # Generate normal data points around a center
    normal_data = np.random.randn(n_normals, dimensions)

    # Calculate the rescaling factor for higher dimensions
    rescaling_factor = np.sqrt(2 / dimensions)  # sqrt(2/dimensions) to match the 2D scaling

    # Rescale the outlier_distance
    rescaled_outlier_distance = outlier_distance * rescaling_factor

    # Generate outliers with rescaled distance
    outlier_data = np.random.randn(n_outliers, dimensions) * rescaled_outlier_distance + rescaled_outlier_distance

    # Combine normal data and outliers
    data = np.vstack([normal_data, outlier_data])

    # Create labels (0 for normal, 1 for outlier)
    labels = np.array([0] * n_normals + [1] * n_outliers)

    return data, labels

# Example usage for 50 dimensions with rescaling
data, labels = generate_simulation_data(n_points=2280, outlier_ratio=0.05, dimensions=50, outlier_distance=10)

random.seed(5)
all_indices = list(range(0, 2280))
selected_index = random.sample(all_indices, 1000)
unselected_index = [index for index in all_indices if index not in selected_index]
random.shuffle(unselected_index)
# Use selected_index and unselected_index to get selected_data, unselected_data, selected_labels, and unselected_labels
selected_data = data[selected_index]
selected_labels = labels[selected_index]
unselected_data = data[unselected_index]
unselected_labels = labels[unselected_index]

test_index = range(0,1280)
indices = [1,4,9,19,39,79,159,319,639,1279]
f1_total = []
k = [25,50,75,100,125,150]
for kk in k:
    data_copy = selected_data
    lof_list = []
    reach_list = []
    lrd_list = []
    dist_og = cdist(selected_data, selected_data, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, kk)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, kk]
    reachability_dist = reachability_distance(dist_og, kk) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, kk)
    f1_new_list = []
    for i in test_index:
        new_point = unselected_data[i]
        lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, kk, new_point, reachability_dist)
        data_copy = np.append(data_copy, [new_point], axis = 0)
        lof_mean_incremental = np.mean(lof_og)
        lof_std_inc = np.std(lof_og)
        threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
        # Classify points based on the threshold
        predicted_labels = (lof_og > threshold_incremental_new).astype(int)
        labels_update = np.append(selected_labels, unselected_labels[0:i+1])
        # Calculate F1 score and accuracy
        f1 = f1_score(labels_update, predicted_labels)
        f1_new_list.append(f1)
    selected_values = [f1_new_list[i] for i in indices]
    f1_total.append(selected_values)

f1_total

test_index = range(0,640)
f1_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

k = 50
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


chunk_size = 320 # Adjust this based on your dataset size and memory capacity

# Initialize lists to keep track of metrics
f1_list_first_half = []
accuracy_list_first_half = []

# Process the first half
for i in range(chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = lof_mean_incremental + 2 * lof_std_inc

    # Predict and update labels
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate metrics
    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    # Store metrics
    f1_list_first_half.append(f1_inc)
    accuracy_list_first_half.append(accuracy_inc)

# Initialize lists for the second half metrics if needed
f1_list_second_half = []
accuracy_list_second_half = []

# Process the second half
for i in range(chunk_size, 2 * chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = lof_mean_incremental + 2 * lof_std_inc

    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    f1_list_second_half.append(f1_inc)
    accuracy_list_second_half.append(accuracy_inc)

# Combining the two F1 score lists from the first and second half
f1_list = f1_list_first_half + f1_list_second_half

# Combined lists
f1_list

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 50
f1_new_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list.append(f1)
f1_new_list

import matplotlib.pyplot as plt
from google.colab import files


# Number of increments (x-axis)
x = range(len(f1_list))

# Select key points every 20th point for clarity
key_points = np.arange(0, len(f1_list), 20)

# Plotting the data
plt.figure(figsize=(10, 6))

# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list)[key_points], label='ILOF', marker='s', color='blue', markersize=7, linestyle='-', linewidth=1.5)
plt.plot(key_points, np.array(f1_new_list)[key_points], label='EILOF', marker='s', color='green', linestyle='-', markersize=7, linewidth=1.5)


# Adding labels, legend, and grid
plt.xlabel('Incremental Points', fontsize=12)
plt.ylabel('F1 Score', fontsize=12)
plt.legend(fontsize=12)
plt.grid(True)
plt.tight_layout()
plt.savefig('incremental_f1_scores_comparison.png', dpi=300)

# Show plot
plt.show()
files.download('incremental_f1_scores_comparison.png')

"""Appling real dataset (shuttle data) https://archive.ics.uci.edu/dataset/148/statlog+shuttle

exclude class 4 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4836738/
"""

!pip install ucimlrepo
from ucimlrepo import fetch_ucirepo
import pandas as pd

# Fetch the dataset
statlog_shuttle = fetch_ucirepo(id=148)

# Data (as pandas dataframes)
X = statlog_shuttle.data.features
y = statlog_shuttle.data.targets

# Combine X and y into a single DataFrame for easier filtering
data = X.copy()
data['class'] = y

# Remove rows where y (class) equals 4
data_filtered = data[data['class'] != 4]

# Separate X and y after filtering
X = data_filtered.drop(columns=['class'])
data_filtered['class'] = data_filtered['class'].apply(lambda x: 0 if x == 1 else 1)
y = data_filtered['class']

X

# Use selected_index and unselected_index to get selected_data, unselected_data, selected_labels, and unselected_labels
selected_data = X[:1000].reset_index(drop=True).values
selected_labels = y[:1000].reset_index(drop=True).values
unselected_data = X[1000:2280].reset_index(drop=True).values
unselected_labels = y[1000:2280].reset_index(drop=True).values

"""ILOF set threshold to 7%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
f1_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

k = 100
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


chunk_size = 320 # Adjust this based on your dataset size and memory capacity

# Initialize lists to keep track of metrics
f1_list_first_half = []
accuracy_list_first_half = []

# Process the first half
for i in range(chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = np.percentile(lof_og, 93)

    # Predict and update labels
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate metrics
    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    # Store metrics
    f1_list_first_half.append(f1_inc)
    accuracy_list_first_half.append(accuracy_inc)

# Initialize lists for the second half metrics if needed
f1_list_second_half = []
accuracy_list_second_half = []

# Process the second half
for i in range(chunk_size, 2 * chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = np.percentile(lof_og, 93)

    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    f1_list_second_half.append(f1_inc)
    accuracy_list_second_half.append(accuracy_inc)

# Combining the two F1 score lists from the first and second half
f1_list = f1_list_first_half + f1_list_second_half

# Now you can use these combined lists for further analysis or reporting
f1_list

k = 100
data_copy = selected_data
selected_data = np.vstack((selected_data,unselected_data[:299]))
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

"""EILOF set the threshold to 7%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 100
f1_new_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 93)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list.append(f1)
f1_new_list

from sklearn.metrics import f1_score, accuracy_score
import numpy as np
from scipy.spatial.distance import cdist

# Define the range of k values
k_values = range(50, 150)
f1_scores = []
accuracy_scores = []

# Compute the pairwise distances
data_1600 = np.vstack((selected_data,unselected_data[:640]))
selected_labels_1600 = np.append(selected_labels,unselected_labels[:640])
dist_og = cdist(data_1600, data_1600, 'euclidean')

for k in k_values:
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k)  # original reachability dist
    lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # original lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 90)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_scores.append(f1)
    accuracy_scores.append((k, accuracy))

f1_scores

"""ILOF set the threshold to 5%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
f1_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

k = 100
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


chunk_size = 320 # Adjust this based on your dataset size and memory capacity

# Initialize lists to keep track of metrics
f1_list_first_half = []
accuracy_list_first_half = []

# Process the first half
for i in range(chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = np.percentile(lof_og, 95)

    # Predict and update labels
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate metrics
    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    # Store metrics
    f1_list_first_half.append(f1_inc)
    accuracy_list_first_half.append(accuracy_inc)

# Initialize lists for the second half metrics if needed
f1_list_second_half = []
accuracy_list_second_half = []

# Process the second half
for i in range(chunk_size, 2 * chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = np.percentile(lof_og, 95)

    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    f1_list_second_half.append(f1_inc)
    accuracy_list_second_half.append(accuracy_inc)

# Combining the two F1 score lists from the first and second half
f1_list = f1_list_first_half + f1_list_second_half

# Now you can use these combined lists for further analysis or reporting
f1_list

"""EILOF set the threshold to 5%

"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 100
f1_new_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 95)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list.append(f1)
f1_new_list

"""ILOF set the threshold to 10%"""

test_index = range(0,640)
f1_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

k = 100
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


chunk_size = 320 # Adjust this based on your dataset size and memory capacity

# Initialize lists to keep track of metrics
f1_list_first_half = []
accuracy_list_first_half = []

# Process the first half
for i in range(chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = np.percentile(lof_og, 90)

    # Predict and update labels
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate metrics
    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    # Store metrics
    f1_list_first_half.append(f1_inc)
    accuracy_list_first_half.append(accuracy_inc)

f1_list_first_half

# Initialize lists for the second half metrics if needed
f1_list_second_half = []
accuracy_list_second_half = []

# Process the second half
for i in range(chunk_size, 2 * chunk_size):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    threshold_incremental = np.percentile(lof_og, 90)

    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    f1_inc = f1_score(labels_update_inc, predicted_labels_inc)
    accuracy_inc = accuracy_score(labels_update_inc, predicted_labels_inc)

    f1_list_second_half.append(f1_inc)
    accuracy_list_second_half.append(accuracy_inc)

# Combining the two F1 score lists from the first and second half
f1_list = f1_list_first_half + f1_list_second_half

# Now you can use these combined lists for further analysis or reporting
f1_list

"""EILOF set the threshold to 10%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 100
f1_new_list = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 90)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list.append(f1)
f1_new_list

import numpy as np
import matplotlib.pyplot as plt

# Number of increments (x-axis)
x = range(len(f1_list7))

# Select key points every 20th point for clarity
key_points = np.insert(np.arange(19, len(f1_list7), 20), 0, 0)

# Create a high-resolution figure
plt.figure(figsize=(12, 8), dpi=300)  # Increased figure size and resolution (DPI)

# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list7)[key_points], label='ILOF 7%', marker='o', color='blue', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list7)[key_points], label='EILOF 7%', marker='s', color='blue', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 5% and EILOF 5% with different markers
plt.plot(key_points, np.array(f1_list5)[key_points], label='ILOF 5%', marker='o', color='red', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list5)[key_points], label='EILOF 5%', marker='s', color='red', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 10% and EILOF 10% with different markers
plt.plot(key_points, np.array(f1_list10)[key_points], label='ILOF 10%', marker='o', color='green', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list10)[key_points], label='EILOF 10%', marker='s', color='green', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Adding labels, legend, and grid with better formatting
plt.xlabel('Incremental Points', fontsize=20)
plt.ylabel('F1 Score', fontsize=20)
plt.legend(fontsize=18, loc='lower left')
plt.xticks(fontsize=16)  # Increase font size for x-axis scale numbers
plt.yticks(fontsize=16)  # Increase font size for y-axis scale numbers
plt.grid(True)
# Tight layout and export settings
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300)  # Save the figure as a high-resolution PNG
plt.show()

"""Fig. 9 (b):Comparison of EILOF and ILOF performance on Shuttle dataset when $k = 100$

when k = 150, ILOF 10%
"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 150
f1_list_10_k150 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []


for i in np.insert(np.arange(19, len(test_index), 20), 0, 0):
    data_copy = np.concatenate((selected_data, unselected_data[:i]))
    dist_og = cdist(data_copy, data_copy, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 90)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i]))

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_list_10_k150.append(f1)

f1_list_10_k150

"""EILOF k = 150, 10% of the outliers"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 150
f1_new_list_10_k150 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 90)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list_10_k150.append(f1)
f1_new_list_10_k150

"""when k = 150, ILOF 7%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 150
f1_list_7_k150 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []


for i in np.insert(np.arange(19, len(test_index), 20), 0, 0):
    data_copy = np.concatenate((selected_data, unselected_data[:i]))
    dist_og = cdist(data_copy, data_copy, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 93)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i]))

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_list_7_k150.append(f1)
f1_list_7_k150

# from sklearn.metrics import f1_score, accuracy_score
# test_index = range(0,640)
# k = 150
# f1_list_7_k150 = []
# Accuracy = []
# data_copy = selected_data
# lof_list = []
# reach_list = []
# lrd_list = []


# for i in listtt:
#     data_copy = np.concatenate((selected_data, unselected_data[:i+1]))
#     dist_og = cdist(data_copy, data_copy, 'euclidean')
#     neighbors_og = k_nearest_neighbors(dist_og, k)
#     k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
#     reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
#     lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
#     lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
#     # Calculate the threshold as the 93rd percentile of LOF values
#     threshold_incremental_new = np.percentile(lof_og, 93)

#     # Classify points based on the threshold
#     predicted_labels = (lof_og > threshold_incremental_new).astype(int)
#     selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i+1]))

#     # Calculate F1 score and accuracy
#     f1 = f1_score(selected_labels_1600, predicted_labels)
#     accuracy = accuracy_score(selected_labels_1600, predicted_labels)

#     # Store the results
#     f1_list_7_k150.append(f1)
# f1_list_7_k150

"""when k = 150, EILOF 7%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 150
f1_new_list_7_k150 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 93)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list_7_k150.append(f1)
f1_new_list_7_k150



"""when k = 150, ILOF 5%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 150
f1_list_5_k150 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []


for i in np.insert(np.arange(19, len(test_index), 20), 0, 0):
    data_copy = np.concatenate((selected_data, unselected_data[:i]))
    dist_og = cdist(data_copy, data_copy, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 95)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i]))

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_list_5_k150.append(f1)
f1_list_5_k150

"""when k = 150, EILOF 5%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 150
f1_new_list_5_k150 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 95)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list_5_k150.append(f1)
f1_new_list_5_k150



"""Fig. 9 (b):Comparison of EILOF and ILOF performance on Shuttle dataset when $k = 150$"""

import matplotlib.pyplot as plt
import numpy as np

# Number of increments (x-axis)
x = range(len(f1_new_list_5_k150))

# Select key points every 20th point for clarity
key_points = np.insert(np.arange(19, len(f1_new_list_7_k150), 20), 0, 0)

# Create a high-resolution figure
plt.figure(figsize=(12, 8), dpi=300)  # Increased figure size and resolution (DPI)

# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list_7_k150), label='ILOF 7%', marker='o', color='blue', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list_7_k150)[key_points], label='EILOF 7%', marker='s', color='blue', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 5% and EILOF 5% with different markers
plt.plot(key_points, np.array(f1_list_5_k150), label='ILOF 5%', marker='o', color='red', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list_5_k150)[key_points], label='EILOF 5%', marker='s', color='red', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 10% and EILOF 10% with different markers
plt.plot(key_points, np.array(f1_list_10_k150), label='ILOF 10%', marker='o', color='green', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list_10_k150)[key_points], label='EILOF 10%', marker='s', color='green', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Adding labels, legend, and grid with better formatting
plt.xlabel('Incremental Points', fontsize=20)
plt.ylabel('F1 Score', fontsize=20)
plt.legend(fontsize=18, loc='lower left')
plt.xticks(fontsize=16)  # Increase font size for x-axis scale numbers
plt.yticks(np.arange(0.5, 0.9, 0.05), fontsize=16)
plt.grid(True)
# Tight layout and export settings
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300)  # Save the figure as a high-resolution PNG
plt.show()

"""When k = 50, outlier ratio = 7% ILOF"""

test_index = range(0,640)
k = 50
f1_list_7_k50 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []


for i in np.insert(np.arange(19, len(test_index), 20), 0, 0):
    data_copy = np.concatenate((selected_data, unselected_data[:i]))
    dist_og = cdist(data_copy, data_copy, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 93)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i]))

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_list_7_k50.append(f1)
f1_list_7_k50

"""When k= 50, EILOF 7%"""

test_index = range(0,640)
k = 50
f1_new_list_7_k50 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 93)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list_7_k50.append(f1)
f1_new_list_7_k50

"""When k = 50, outlier ratio = 10% ILOF"""

test_index = range(0,640)
k = 50
f1_list_10_k50 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []


for i in np.insert(np.arange(19, len(test_index), 20), 0, 0):
    data_copy = np.concatenate((selected_data, unselected_data[:i]))
    dist_og = cdist(data_copy, data_copy, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 90)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i]))

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_list_10_k50.append(f1)
f1_list_10_k50

"""When k= 50, EILOF 10%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 50
f1_new_list_10_k50 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 90)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list_10_k50.append(f1)
f1_new_list_10_k50

"""When k = 50, outlier ratio = 5% ILOF"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 50
f1_list_5_k50 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []


for i in np.insert(np.arange(19, len(test_index), 20), 0, 0):
    data_copy = np.concatenate((selected_data, unselected_data[:i]))
    dist_og = cdist(data_copy, data_copy, 'euclidean')
    neighbors_og = k_nearest_neighbors(dist_og, k)
    k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
    reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
    lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
    lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)
    # Calculate the threshold as the 93rd percentile of LOF values
    threshold_incremental_new = np.percentile(lof_og, 95)

    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    selected_labels_1600 = np.concatenate((selected_labels, unselected_labels[:i]))

    # Calculate F1 score and accuracy
    f1 = f1_score(selected_labels_1600, predicted_labels)
    accuracy = accuracy_score(selected_labels_1600, predicted_labels)

    # Store the results
    f1_list_5_k50.append(f1)
f1_list_5_k50

"""When k= 50, EILOF 5%"""

from sklearn.metrics import f1_score, accuracy_score
test_index = range(0,640)
k = 50
f1_new_list_5_k50 = []
Accuracy = []
data_copy = selected_data
lof_list = []
reach_list = []
lrd_list = []

dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k) #orginal reachability dis
lrd_og = local_reachability_density(reachability_dist, neighbors_og) #orginal lrd
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)


for i in test_index:
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof_update_new(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis = 0)
    lof_mean_incremental = np.mean(lof_og)
    lof_std_inc = np.std(lof_og)
    #threshold_incremental_new = lof_mean_incremental + 2 * lof_std_inc
    threshold_incremental_new = np.percentile(lof_og, 95)
    # Classify points based on the threshold
    predicted_labels = (lof_og > threshold_incremental_new).astype(int)
    labels_update = np.append(selected_labels, unselected_labels[0:i+1])

    # Calculate F1 score and accuracy
    f1 = f1_score(labels_update, predicted_labels)
    accuracy = accuracy_score(labels_update, predicted_labels)
    f1_new_list_5_k50.append(f1)
f1_new_list_5_k50

"""Fig. 9 (b):Comparison of EILOF and ILOF performance on Shuttle dataset when $k = 50$"""

import matplotlib.pyplot as plt
import numpy as np

# Number of increments (x-axis)
x = range(len(f1_new_list_5_k150))

# Select key points every 20th point for clarity
key_points = np.insert(np.arange(19, len(f1_list7), 20), 0, 0)

# Create a high-resolution figure
plt.figure(figsize=(12, 8), dpi=300)  # Increased figure size and resolution (DPI)

# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list_7_k50), label='ILOF 7%', marker='o', color='blue', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list_7_k50)[key_points], label='EILOF 7%', marker='s', color='blue', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 5% and EILOF 5% with different markers
plt.plot(key_points, np.array(f1_list_5_k50), label='ILOF 5%', marker='o', color='red', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list_5_k50)[key_points], label='EILOF 5%', marker='s', color='red', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 10% and EILOF 10% with different markers
plt.plot(key_points, np.array(f1_list_10_k50), label='ILOF 10%', marker='o', color='green', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list_10_k50)[key_points], label='EILOF 10%', marker='s', color='green', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Adding labels, legend, and grid with better formatting
plt.xlabel('Incremental Points', fontsize=20)
plt.ylabel('F1 Score', fontsize=20)
plt.legend(fontsize=18, loc='lower left')
plt.xticks(fontsize=16)  # Increase font size for x-axis scale numbers
plt.yticks(np.arange(0.10, 0.60, 0.05), fontsize=16)
plt.grid(True)
# Tight layout and export settings
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300)  # Save the figure as a high-resolution PNG
plt.show()

"""## Credit Card Fraud: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")

print("Path to dataset files:", path)

import pandas as pd

# Assuming a CSV file is in the dataset directory
csv_file = os.path.join(path, "creditcard.csv")  # Replace with the actual file name
data = pd.read_csv(csv_file)

# Display the first few rows of the dataset
data.head()

rimport pandas as pd

# Step 1: Identify outliers and non-outliers
outliers = data[data['Class'] == 1]
non_outliers = data[data['Class'] == 0]

# Step 2: Calculate the number of non-outliers required
n_outliers = len(outliers)  # Total number of outliers
required_total = n_outliers / 0.05  # Total rows needed for 5% outliers
required_non_outliers = int(required_total - n_outliers)

# Step 3: Randomly sample the required number of non-outliers
non_outliers_sampled = non_outliers.sample(n=required_non_outliers, random_state=42)

# Step 4: Combine outliers and sampled non-outliers
subset = pd.concat([outliers, non_outliers_sampled])

# Step 5: Maintain time-series order
subset = subset.sort_index()

# Step 6: Verify the outlier ratio
actual_outlier_ratio = len(subset[subset['Class'] == 1]) / len(subset)
print(f"Outlier Ratio in Subset: {actual_outlier_ratio:.2%}")
# Display the subset
subset.head()

from sklearn.preprocessing import StandardScaler
data = subset.drop(columns=['Class'])
label = subset["Class"]

# Initialize selected and unselected datasets
selected_data = data[:1000].reset_index(drop=True).values
selected_labels = label[:1000].reset_index(drop=True).values
unselected_data = data[1000:2280].reset_index(drop=True).values
unselected_labels = label[1000:2280].reset_index(drop=True).values
scaler = StandardScaler()
selected_data = scaler.fit_transform(selected_data)
scaler = StandardScaler()
unselected_data = scaler.fit_transform(unselected_data)



"""# ILOF threshold = 5% Credit Card k = 100"""

data_copy = selected_data
k = 100

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 95)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

"""# ILOF threshold = 7% Credit Card k = 100"""

data_copy = selected_data
k = 100

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 93)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list



"""# ILOF threshold = 10% Credit Card k = 100"""

data_copy = selected_data
k = 100

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 90)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

"""# ELOF threshold = 5% Credit Card k = 100"""

data_copy = selected_data
k = 100

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 95)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# ELOF threshold = 7% Credit Card k = 100"""

data_copy = selected_data
k = 100

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 93)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# ELOF threshold = 10% Credit Card k = 100"""

data_copy = selected_data
k = 100

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 90)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

# Number of increments (x-axis)
x = range(len(f1_list7))

# Select key points every 20th point for clarity
key_points = np.insert(np.arange(19, len(f1_list7), 20), 0, 0)

# Create a high-resolution figure
plt.figure(figsize=(12, 8), dpi=300)

# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list7)[key_points], label='ILOF 7%', marker='o', color='blue', markersize=8, linestyle='-', linewidth=1.5)
plt.plot(key_points, np.array(f1_new_list7)[key_points], label='EILOF 7%', marker='s', color='blue', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 5% and EILOF 5% with different markers
plt.plot(key_points, np.array(f1_list5)[key_points], label='ILOF 5%', marker='o', color='red', markersize=8, linestyle='-', linewidth=1.5)
plt.plot(key_points, np.array(f1_new_list5)[key_points], label='EILOF 5%', marker='s', color='red', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 10% and EILOF 10% with different markers
plt.plot(key_points, np.array(f1_list10)[key_points], label='ILOF 10%', marker='o', color='green', markersize=8, linestyle='-', linewidth=1.5)
plt.plot(key_points, np.array(f1_new_list10)[key_points], label='EILOF 10%', marker='s', color='green', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Adding labels, legend, and grid with better formatting
plt.xlabel('Incremental Points', fontsize=20)
plt.ylabel('F1 Score', fontsize=20)
plt.legend(fontsize=18, loc='lower left')
plt.xticks(fontsize=16)  # Increase font size for x-axis scale numbers
plt.yticks(np.arange(0.20, 0.85, 0.05), fontsize=16)
plt.grid(True)
# Tight layout and export settings
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300)  # Save the figure as a high-resolution PNG
plt.show()

"""# EILOF k = 150 5% outliers"""

data_copy = selected_data
k = 150

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 95)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# EILOF k = 150 7% outliers"""

data_copy = selected_data
k = 150

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 93)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# EILOF k = 150 10% outliers"""

data_copy = selected_data
k = 150

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 90)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# When k =150 threshold = 5% ILOF"""

data_copy = selected_data
k = 150

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 95)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

"""# When k =150 threshold = 7% ILOF"""

data_copy = selected_data
k = 150

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 93)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

"""# When k =150 threshold = 10% ILOF"""

data_copy = selected_data
k = 150

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 90)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

# Number of increments (x-axis)
x = range(len(f1_list7))

# Select key points every 20th point for clarity
key_points = np.insert(np.arange(19, len(f1_list7), 20), 0, 0)

# Create a high-resolution figure
plt.figure(figsize=(12, 8), dpi=300)
# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list7)[key_points], label='ILOF 7%', marker='o', color='blue', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list7)[key_points], label='EILOF 7%', marker='s', color='blue', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 5% and EILOF 5% with different markers
plt.plot(key_points, np.array(f1_list5)[key_points], label='ILOF 5%', marker='o', color='red', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list5)[key_points], label='EILOF 5%', marker='s', color='red', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 10% and EILOF 10% with different markers
plt.plot(key_points, np.array(f1_list10)[key_points], label='ILOF 10%', marker='o', color='green', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list10)[key_points], label='EILOF 10%', marker='s', color='green', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)


# Adding labels, legend, and grid with better formatting
plt.xlabel('Incremental Points', fontsize=20)
plt.ylabel('F1 Score', fontsize=20)
plt.legend(fontsize=18, loc='lower left')
plt.xticks(fontsize=16)  # Increase font size for x-axis scale numbers
plt.yticks(np.arange(0.25, 0.90, 0.05), fontsize=16)
plt.grid(True)
# Tight layout and export settings
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300)  # Save the figure as a high-resolution PNG
plt.show()

"""# EILOF 5% Outliers k = 50"""

data_copy = selected_data
k = 50

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 95)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# EILOF 7% Outliers k = 50"""

data_copy = selected_data
k = 50

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 93)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# EILOF 10% Outliers k = 50"""

data_copy = selected_data
k = 50

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og, dist_og= incremental_lof_update_new(data_copy, k, new_point, reachability_dist, dist_og)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 90)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))
f1_list

"""# When k =50 threshold = 5% ILOF"""

data_copy = selected_data
k = 50

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 95)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

"""# When k =50 threshold = 7% ILOF"""

data_copy = selected_data
k = 50

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 93)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

"""# When k =50 threshold = 10% ILOF"""

data_copy = selected_data
k = 50

# Calculate original LOF metrics
dist_og = cdist(selected_data, selected_data, 'euclidean')
neighbors_og = k_nearest_neighbors(dist_og, k)
k_th_dist_og = np.sort(dist_og, axis=1)[:, k]
reachability_dist = reachability_distance(dist_og, k)  # Original reachability distance
lrd_og = local_reachability_density(reachability_dist, neighbors_og)  # Original LRD
lof_og = lof_srs(dist_og, neighbors_og, lrd_og, k)

# Initialize lists for metrics
f1_list = []
accuracy_list = []

# Process the unselected data incrementally
for i in range(640):
    new_point = unselected_data[i]
    lof_og, reachability_dist, lrd_og = incremental_lof(data_copy, k, new_point, reachability_dist)
    data_copy = np.append(data_copy, [new_point], axis=0)

    # Update LOF metrics and thresholds
    threshold_incremental = np.percentile(lof_og, 90)
    predicted_labels_inc = (lof_og > threshold_incremental).astype(int)
    labels_update_inc = np.append(selected_labels, unselected_labels[:i+1])

    # Calculate and store metrics
    f1_list.append(f1_score(labels_update_inc, predicted_labels_inc))
    accuracy_list.append(accuracy_score(labels_update_inc, predicted_labels_inc))

    # Print progress every 50 iterations
    if i % 50 == 0:
        print(f"Progress: {i}/{640} iterations completed.")

# Final output
print("Processing complete.")
f1_list

# Number of increments (x-axis)
x = range(len(f1_list7))

# Select key points every 20th point for clarity
key_points = np.insert(np.arange(19, len(f1_list7), 20), 0, 0)

# Create a high-resolution figure
plt.figure(figsize=(12, 8), dpi=300)
# Plotting ILOF 7% and EILOF 7% with different markers
plt.plot(key_points, np.array(f1_list7)[key_points], label='ILOF 7%', marker='o', color='blue', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list7)[key_points], label='EILOF 7%', marker='s', color='blue', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 5% and EILOF 5% with different markers
plt.plot(key_points, np.array(f1_list5)[key_points], label='ILOF 5%', marker='o', color='red', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list5)[key_points], label='EILOF 5%', marker='s', color='red', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)

# Plotting ILOF 10% and EILOF 10% with different markers
plt.plot(key_points, np.array(f1_list10)[key_points], label='ILOF 10%', marker='o', color='green', markersize=8, linestyle='-', linewidth=2)
plt.plot(key_points, np.array(f1_new_list10)[key_points], label='EILOF 10%', marker='s', color='green', linestyle='dashed', markersize=8, markerfacecolor='none', linewidth=2)


# Adding labels, legend, and grid with better formatting
plt.xlabel('Incremental Points', fontsize=20)
plt.ylabel('F1 Score', fontsize=20)
plt.legend(fontsize=18, loc='lower left')
plt.xticks(fontsize=16)  # Increase font size for x-axis scale numbers
plt.yticks(np.arange(0, 0.55, 0.05), fontsize=16)
plt.grid(True)
# Tight layout and export settings
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300)  # Save the figure as a high-resolution PNG
plt.show()

